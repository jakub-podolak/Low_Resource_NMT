{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuX1gxFbh1P-"
   },
   "source": [
    "# NLP 2 Project: Backtranslation for Domain Adaptation\n",
    "\n",
    "In this project, you will fine-tune a translation model by backtranslating monolingual in-domain text. You will then test performance in that domain as well as general domains.\n",
    "\n",
    "Your first task is to compare fine-tuning with backtranslation.\n",
    "Next, you will explore a method of data selection.\n",
    "Third, you will extend backtranslation, either modifying decoding, the model, or using multilingual pivots.\n",
    "Finally, you will explore your own research question.\n",
    "\n",
    "This notebook provides starter code to preprocess, fine-tune, and generate with a translation model. This is enough to get you started on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "# import vllm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDnACDAIjMay"
   },
   "source": [
    "## Preprocessing\n",
    "First, we need to tokenize our inputs. With HF Transformers, this is fairly simple and is done for you below. Here, we use the model's tokenizer to split the inputs into the model's pre-defined numerical tokens, i.e. convert text into tensors. We also need a function to convert back from tensors into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.preprocessing import preprocess_data, postprocess_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_kEixCZjyL-"
   },
   "source": [
    "## Evaluation\n",
    "During fine-tuning, we need to see how good the outputs are on our dev set. For this, we can use BLEU score (Papineni 2002). This function decodes the predicted tensor tokens, and computes the BLEU score.\n",
    "\n",
    "On our test sets, we also want to calculate an automatic metric, but on decoded text. We can use BLEU again, but also more advanced metrics like COMET. It's up to you to implement your choice of metric. We will discuss some metrics from the literature in class. It's always good to use at least 2 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.metrics import compute_comet, compute_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc8HvRkkogB"
   },
   "source": [
    "## Fine-tuning\n",
    "Now that we've tokenized our data and got our evaluation ready, we can start fine-tuning (i.e., training from a pre-trained model). This is a minimal training loop.\n",
    "\n",
    "We also need to generate at test time from a text dataset. This function involves generation without calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.training_utils import train_model, translate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwUNfHgylIcR"
   },
   "source": [
    "## Final Setup\n",
    "We now have all the ingredients to run our experiments. This is all standard training code; the interesting results come from what you do with the data. Below, we give an initial setup for getting the code running (either in Colab or on Snellius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nUi-Xho5hwWc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7500\n",
      "Dev dataset size: 1000\n",
      "Test dataset size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SRC_LANG = \"en\"\n",
    "TGT_LANG = \"ru\"\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "TRAIN_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "DEV_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "TEST_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "OUTPUT_DIR = \"./results/checkpoints\"\n",
    "\n",
    "train_dataset = load_dataset(TRAIN_DATASET_NAME)\n",
    "dev_dataset = load_dataset(DEV_DATASET_NAME)\n",
    "test_dataset = load_dataset(TEST_DATASET_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# change the splits for actual training. here, using flores-dev as training set because it's small (<1k examples)\n",
    "tokenized_train_dataset = preprocess_data(train_dataset, tokenizer, SRC_LANG, TGT_LANG, \"train\")\n",
    "tokenized_dev_dataset = preprocess_data(dev_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "# Note(jp): Here test is the same as dev\n",
    "tokenized_test_dataset = preprocess_data(test_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "\n",
    "# print sizes\n",
    "print(f\"Train dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(tokenized_dev_dataset)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_test_dataset)}\")\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train_dataset,\n",
    "    \"dev\": tokenized_dev_dataset,\n",
    "    \"test\": tokenized_test_dataset\n",
    "})\n",
    "\n",
    "# {'learning_rate': 0.0001, 'weight_decay': 0.05, 'num_train_epochs': 8}\n",
    "# modify these as you wish; RQ3 could involve testing effects of various hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # generally speeds up training, try without it to see if it's faster for small datasets\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64, # change batch sizes to fit your GPU memory and train faster\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    save_total_limit=1, # modify this to save more checkpoints\n",
    "    num_train_epochs=5, # modify this to train more epochs\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=4,\n",
    "    generation_max_length=128,\n",
    "    warmup_steps=0,\n",
    "    no_cuda=False,  # Set to False to enable GPU\n",
    "    fp16=True,      # Enable mixed precision training for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0lfvlbh0lory"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home2/scur2189/Low_Resource_NMT/lib/training_utils.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='590' max='590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [590/590 08:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.070338</td>\n",
       "      <td>2.415048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.752223</td>\n",
       "      <td>6.515725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657745</td>\n",
       "      <td>8.217452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.616448</td>\n",
       "      <td>9.232796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.052800</td>\n",
       "      <td>0.611389</td>\n",
       "      <td>10.060155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fine-tune model\n",
    "model_finetuned = train_model(MODEL_NAME, tokenized_datasets, tokenizer, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on TICO test 10 for baseline and around 14 for fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|                                                                                                          | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:08<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test baseline model\n",
    "predictions = translate_text(test_dataset[\"dev\"][SRC_LANG], model, tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [01:29<00:00,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate checkpoint\n",
    "# checkpoint_model_path = \"results/checkpoints/final_model/best-tuned-opus-mt-en-ru\"\n",
    "\n",
    "# checkpoint_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_model_path)\n",
    "# checkpoint_tokenizer = AutoTokenizer.from_pretrained(checkpoint_model_path)\n",
    "\n",
    "predictions_finetuned = translate_text(test_dataset[\"dev\"][SRC_LANG], model_finetuned,\n",
    "                                       tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and are you having any of the following symptoms with your chest pain \n",
      "и имеются ли у вас сейчас какие-либо из следующих симптомов при боли в грудной клетке\n",
      "и вы имеете следующие симптомы, которые вы говорите: \"Говорите по волшебной груди\" и \"Глубина\" - по волшебной (Глубинной) (Главной) (по волшебной) виски (по волшебной) (по волшебной) виски (по волшебной) в величии (по волшебной) в величии (по величию) (по величию) (по в\n",
      "В хронической группе больных в грудной группе препаратов с хронической симптоматической положительностью и хроническими симпт\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[\"dev\"][SRC_LANG][0])\n",
    "print(test_dataset[\"dev\"][TGT_LANG][0])\n",
    "print(predictions[0])\n",
    "print(predictions_finetuned[0])\n",
    "# В случаях более тяжелого термального воздействия, при костной ткани хорошо выраженные признаки термического воздействия, при"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for baseline model: 6.714642882868273\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bleu_finetuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bleu_finetuned = compute_bleu(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#         test_dataset[\"dev\"][TGT_LANG],\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         predictions_finetuned)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score for baseline model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_baseline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score for fine-tuned model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbleu_finetuned\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# WMT\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# BLEU score for baseline model: 6.714642882868273\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# BLEU score for fine-tuned model: 6.379819414189476\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_finetuned' is not defined"
     ]
    }
   ],
   "source": [
    "bleu_baseline = compute_bleu(\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "\n",
    "bleu_finetuned = compute_bleu(\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions_finetuned)\n",
    "\n",
    "print(f\"BLEU score for baseline model: {bleu_baseline}\")\n",
    "print(f\"BLEU score for fine-tuned model: {bleu_finetuned}\")\n",
    "\n",
    "# WMT\n",
    "# BLEU score for baseline model: 6.714642882868273\n",
    "# BLEU score for fine-tuned model: 6.379819414189476\n",
    "\n",
    "# TICO\n",
    "# BLEU score for baseline model: 12.522936666648125\n",
    "# BLEU score for fine-tuned model: 3.078057409647383\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07df7888125a495889c4c0691d37d0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2189/.conda/envs/nmt/lib/python3.10/site-p ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3548f6c2ed74babb9c4700725e06097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2189/.conda/envs/nmt/lib/python3.10/site-p ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET score for baseline model: 0.5964977323671325\n",
      "COMET score for fine-tuned model: 0.4390196953080096\n"
     ]
    }
   ],
   "source": [
    "comet_baseline = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "\n",
    "comet_finetuned = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions_finetuned)\n",
    "\n",
    "print(f\"COMET score for baseline model: {comet_baseline}\")\n",
    "print(f\"COMET score for fine-tuned model: {comet_finetuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the cases of a more rigorous thermal impact when the bone tissue exhibits well pronounced signs of heat destruction, it should be considered as inherently unsuitable for genotyping of mtDNA.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"dev\"][SRC_LANG][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Установили, что хромосомная ДНК уступает в аналитической устойчивости мтДНК.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"dev\"][TGT_LANG][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В случае более жесткого термического удара, когда костная ткань имеет хорошо заметные признаки термического разрушения, ее следует рассматривать как по своей сути непригодную для гнилого гнилого гнильного гнильного гнильного гнильного гнильного гнильного гнилого гнильного гнильного гнильного гнили с гнилью гнилого смолы и гнилого смолы.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В случаях более тяжелого термального воздействия, при костной ткани хорошо выраженные признаки термического воздействия, при'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_finetuned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate==0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep \"evaluate\" # evaluate==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading checkpoint model from: results/checkpoints/final_model/best-tuned-opus-mt-en-ru\n",
      "[INFO] Loading datasets...\n",
      "[INFO] Loading dataset: sethjsa/medline_en_ru_parallel (dev)\n",
      "[INFO] Preprocessing data...\n",
      "[INFO] Translating test data...\n",
      "Translating:   0%|                                       | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|██████████████████████████████| 16/16 [01:25<00:00,  5.32s/it]\n",
      "[INFO] Computing metrics...\n",
      "Fetching 5 files: 100%|████████████████████████| 5/5 [00:00<00:00, 41282.52it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 scripts/evaluate_test.py --checkpoint_path results/ ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[INFO] BLEU score: 6.604879770209201\n",
      "[INFO] COMET score: 0.5222889098227024\n",
      "[INFO] Results:\n",
      "Dataset: sethjsa/medline_en_ru_parallel, Split: dev, Samples: 1000, BLEU: 6.604879770209201, COMET: 0.5222889098227024\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/evaluate_test.py --checkpoint_path results/checkpoints/final_model/best-tuned-opus-mt-en-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading baseline model from: Helsinki-NLP/opus-mt-en-ru\n",
      "[INFO] Loading datasets...\n",
      "[INFO] Loading dataset: sethjsa/medline_en_ru_parallel (dev)\n",
      "[INFO] Preprocessing data...\n",
      "[INFO] Translating test data...\n",
      "Translating:   0%|                                       | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|██████████████████████████████| 16/16 [01:08<00:00,  4.26s/it]\n",
      "[INFO] Computing metrics...\n",
      "Fetching 5 files: 100%|████████████████████████| 5/5 [00:00<00:00, 24188.60it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 scripts/evaluate_test.py ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[INFO] BLEU score: 6.714642882868273\n",
      "[INFO] COMET score: 0.4779623000472784\n",
      "[INFO] Results:\n",
      "Dataset: sethjsa/medline_en_ru_parallel, Split: dev, Samples: 1000, BLEU: 6.714642882868273, COMET: 0.4779623000472784\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/evaluate_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8U1vIgwmE-1"
   },
   "source": [
    "You will find all the datasets for this project under: https://huggingface.co/sethjsa\n",
    "\n",
    "For other models, consider \"Helsinki-NLP/opus-mt-en-ru\" (general MT model), \"glazzova/translation_en_ru\" (tuned on biomedical domain), or \"facebook/m2m100_418M\" (multilingual model with 100 languages -- consider using for multilingual pivot experiments).\n",
    "\n",
    "To read more about the WMT Biomedical test data, see here: https://aclanthology.org/2022.wmt-1.69/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-jzQG5Vlh2X"
   },
   "source": [
    "# Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plzMI5gBmW_5"
   },
   "source": [
    "\n",
    "ONLY if you have GPU hours left and want to generate backtranslations with an LLM, consider using vLLM for faster generation. An example function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGqrOvTrhsms"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if using LLM for generation, consider using vllm for faster generation\n",
    "def translate_text_vllm(texts, model_name, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate texts using vllm for faster generation\n",
    "\n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        model_name: Name or path of the model (str)\n",
    "        tokenizer: Tokenizer object\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for translation\n",
    "    Returns:\n",
    "        translations: List of translated texts\n",
    "    \"\"\"\n",
    "    # Use model_name instead of model object\n",
    "    llm = vllm.LLM(\n",
    "        model=model_name,  # Changed from model to model_name\n",
    "        tokenizer=tokenizer,\n",
    "        tensor_parallel_size=1,\n",
    "        max_num_batched_tokens=max_length * batch_size\n",
    "    )\n",
    "\n",
    "    # Create sampling params\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.0,  # Equivalent to greedy decoding\n",
    "        max_tokens=max_length,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Generate translations in batches\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "\n",
    "        # Extract generated text from outputs\n",
    "        batch_translations = [output.outputs[0].text for output in outputs]\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    return translations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjAsnxOtpCzePND80zC+ew",
   "collapsed_sections": [
    "6-jzQG5Vlh2X"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
