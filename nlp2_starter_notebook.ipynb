{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuX1gxFbh1P-"
   },
   "source": [
    "# NLP 2 Project: Backtranslation for Domain Adaptation\n",
    "\n",
    "In this project, you will fine-tune a translation model by backtranslating monolingual in-domain text. You will then test performance in that domain as well as general domains.\n",
    "\n",
    "Your first task is to compare fine-tuning with backtranslation.\n",
    "Next, you will explore a method of data selection.\n",
    "Third, you will extend backtranslation, either modifying decoding, the model, or using multilingual pivots.\n",
    "Finally, you will explore your own research question.\n",
    "\n",
    "This notebook provides starter code to preprocess, fine-tune, and generate with a translation model. This is enough to get you started on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "# import vllm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDnACDAIjMay"
   },
   "source": [
    "## Preprocessing\n",
    "First, we need to tokenize our inputs. With HF Transformers, this is fairly simple and is done for you below. Here, we use the model's tokenizer to split the inputs into the model's pre-defined numerical tokens, i.e. convert text into tensors. We also need a function to convert back from tensors into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.preprocessing import preprocess_data, postprocess_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_kEixCZjyL-"
   },
   "source": [
    "## Evaluation\n",
    "During fine-tuning, we need to see how good the outputs are on our dev set. For this, we can use BLEU score (Papineni 2002). This function decodes the predicted tensor tokens, and computes the BLEU score.\n",
    "\n",
    "On our test sets, we also want to calculate an automatic metric, but on decoded text. We can use BLEU again, but also more advanced metrics like COMET. It's up to you to implement your choice of metric. We will discuss some metrics from the literature in class. It's always good to use at least 2 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.metrics import compute_comet, compute_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc8HvRkkogB"
   },
   "source": [
    "## Fine-tuning\n",
    "Now that we've tokenized our data and got our evaluation ready, we can start fine-tuning (i.e., training from a pre-trained model). This is a minimal training loop.\n",
    "\n",
    "We also need to generate at test time from a text dataset. This function involves generation without calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.training_utils import train_model, translate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwUNfHgylIcR"
   },
   "source": [
    "## Final Setup\n",
    "We now have all the ingredients to run our experiments. This is all standard training code; the interesting results come from what you do with the data. Below, we give an initial setup for getting the code running (either in Colab or on Snellius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nUi-Xho5hwWc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7500\n",
      "Dev dataset size: 1000\n",
      "Test dataset size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SRC_LANG = \"en\"\n",
    "TGT_LANG = \"ru\"\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "TRAIN_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "DEV_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "TEST_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "OUTPUT_DIR = \"./results/checkpoints\"\n",
    "\n",
    "train_dataset = load_dataset(TRAIN_DATASET_NAME)\n",
    "dev_dataset = load_dataset(DEV_DATASET_NAME)\n",
    "test_dataset = load_dataset(TEST_DATASET_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# change the splits for actual training. here, using flores-dev as training set because it's small (<1k examples)\n",
    "tokenized_train_dataset = preprocess_data(train_dataset, tokenizer, SRC_LANG, TGT_LANG, \"train\")\n",
    "tokenized_dev_dataset = preprocess_data(dev_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "# Note(jp): Here test is the same as dev\n",
    "tokenized_test_dataset = preprocess_data(test_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "\n",
    "# print sizes\n",
    "print(f\"Train dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(tokenized_dev_dataset)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_test_dataset)}\")\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train_dataset,\n",
    "    \"dev\": tokenized_dev_dataset,\n",
    "    \"test\": tokenized_test_dataset\n",
    "})\n",
    "\n",
    "# modify these as you wish; RQ3 could involve testing effects of various hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # generally speeds up training, try without it to see if it's faster for small datasets\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=128, # change batch sizes to fit your GPU memory and train faster\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    save_total_limit=1, # modify this to save more checkpoints\n",
    "    num_train_epochs=1, # modify this to train more epochs\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=4,\n",
    "    generation_max_length=128,\n",
    "    no_cuda=False,  # Set to False to enable GPU\n",
    "    fp16=True,      # Enable mixed precision training for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lfvlbh0lory"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home2/scur2189/Low_Resource_NMT/lib/training_utils.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:07, Epoch 0.98/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine-tune model\n",
    "model_finetuned = train_model(MODEL_NAME, tokenized_datasets, tokenizer, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|                                                                                                       | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:08<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# test baseline model\n",
    "predictions = translate_text(test_dataset[\"dev\"][SRC_LANG], model, tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.714642882868273\n"
     ]
    }
   ],
   "source": [
    "bleu_score = compute_bleu(\n",
    "        # test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecb91cac37249238eb6aa624923c979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749b759917dd4cfe980476d62f78abba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee9a0c7131c471ea8e02e3e4e5a3bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bce54537dbd4d489c19e321c0caabfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd88e823ba740a3b71472bceacda3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/9.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc81683196941bd9b10d6fb87805642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f734dfbdd56c4d0a8ff4d866d03f6982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64077a8b696a455696867ea1489f52f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cc549d3ee448a299d23569d9f9a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b24f0f592ec462cbb4bf2bf3cbb0f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc833b30722f40e396892e8759c457e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Module inputs don't match the expected format.\nExpected format: {'sources': Value(dtype='string', id='sequence'), 'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput sources: ['In the cases of a more rigorous thermal impact when the bone tissue exhibits well pronounced signs of heat destruction, it should be considered as inherently unsuitable for genotyping of mtDNA.', 'It was shown that chromosomal DNA is inferior to mtDNA in terms of heat resistance.  This finding agrees with the currently adopted view, however this advantage of mtDNA is relatively insignificant from the standpoint of genotyping efficiency.', '[The dynamics of the dimensional characteristics of the sella turcica in the subjects above 20 years of age].  The objective of the present study was to determine the biological age of the unidentified dead subjects based on the morphometric characteristics of the sella turcica in comparison with other methods available for the purpose in order to narrow the range of the alleged ages of the human remains being examined.', ..., 'PATIENTS AND METHODS', 'It has been carried out a prospective analysis of treatment of 218 patients with scars of different duration, locations and anatomic areas with the use of CO2-laser for the period 2011-2017.', 'POSAS scale and sonography were used for analysis.'],\nInput predictions: ['–í —Å–ª—É—á–∞–µ –±–æ–ª–µ–µ –∂–µ—Å—Ç–∫–æ–≥–æ —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ —É–¥–∞—Ä–∞, –∫–æ–≥–¥–∞ –∫–æ—Å—Ç–Ω–∞—è —Ç–∫–∞–Ω—å –∏–º–µ–µ—Ç —Ö–æ—Ä–æ—à–æ –∑–∞–º–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä—É—à–µ–Ω–∏—è, –µ–µ —Å–ª–µ–¥—É–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –ø–æ —Å–≤–æ–µ–π —Å—É—Ç–∏ –Ω–µ–ø—Ä–∏–≥–æ–¥–Ω—É—é –¥–ª—è –≥–Ω–∏–ª–æ–≥–æ –≥–Ω–∏–ª–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª–∏ —Å –≥–Ω–∏–ª—å—é –≥–Ω–∏–ª–æ–≥–æ —Å–º–æ–ª—ã –∏ –≥–Ω–∏–ª–æ–≥–æ —Å–º–æ–ª—ã.', '–ë—ã–ª–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ, —á—Ç–æ —Ö—Ä–æ–º–æ—Å–æ–º–Ω–∞—è –î–ù–ö —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ç–µ–ø–ª–æ—Å—Ç–æ–π–∫–æ—Å—Ç–∏ –Ω–∏–∂–µ mtDNA, —á—Ç–æ —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è —Å –ø—Ä–∏–Ω—è—Ç–æ–π –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Ç–æ—á–∫–æ–π –∑—Ä–µ–Ω–∏—è, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ mtden —è–≤–ª—è–µ—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è mut grum grum grum sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub int int int int int it sput sput', '–¶–µ–ª—å –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–ª–∞ –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–∑—Ä–∞—Å—Ç –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –º—ë—Ä—Ç–≤—ã—Ö —Å—É–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ—Ä—Ñ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ü–æ–∫–æ–ª—å–Ω–æ–π –¥—ã—Ä—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –∏–º–µ—é—â–∏–º–∏—Å—è –¥–ª—è —ç—Ç–æ–π —Ü–µ–ª–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Å —Ç–µ–º —á—Ç–æ–±—ã —Å—É–∑–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã—Ö –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø —Ö—Ä–µ–±—Ç–æ–≤ —Ö—Ä–µ–±—Ç–æ–≤–æ–π —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã.', ..., '–°–û–í–ï–©–ê–ù–ò–ï –ü–û –ü–û –ü–û–î–î–ï–†–ñ–ê–ù–ò–Æ –≠–¢–ò–• –ü–û–ú–û–©–¨ –ü–û –ü–û–ú–û–©–¨ –í –°–í–Ø–ó–ò –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –°–í–Ø–ó–Ø–ú –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –ü–û–ê–ú–ê–ú –ü–û –ü–û –°–ê–ú–ê–ú–ê–ú', '–ë—ã–ª –ø—Ä–æ–≤–µ–¥–µ–Ω –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–µ—á–µ–Ω–∏—è 218 –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å —à—Ä–∞–º–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è–º–∏ –∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–º–∏ –∑–æ–Ω–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ –ø–µ—Ä–∏–æ–¥ —Å 2011 –ø–æ 2017 –≥–æ–¥—ã –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö —à—Ä–∞–º–æ–≤ –∏ –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö —á–µ—Ä–µ–ø–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö —á–µ—Ä–µ–ø–∞—Ö.', '–°—Ü–µ–Ω–∞—Ä–∏–π –∏ —Å–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—é –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏.'],\nInput references: [['–£—Å—Ç–∞–Ω–æ–≤–∏–ª–∏, —á—Ç–æ —Ö—Ä–æ–º–æ—Å–æ–º–Ω–∞—è –î–ù–ö —É—Å—Ç—É–ø–∞–µ—Ç –≤ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º—Ç–î–ù–ö.'], ['–≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —É—Å—Ç–æ—è–≤—à–∏–º—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º, –æ–¥–Ω–∞–∫–æ, —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–µ–Ω–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è, —ç—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –º—Ç–î–ù–ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–≤–µ–ª–∏–∫–æ.'], ['–¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è - —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ø–æ–≥–∏–±—à–∏—Ö –ª–∏—Ü —Å –ø–æ–º–æ—â—å—é –º–æ—Ä—Ñ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —Å–µ–¥–ª–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Ö —Å —É–∂–µ –∏–º–µ—é—â–∏–º–∏—Å—è –º–µ—Ç–æ–¥–∞–º–∏, —á—Ç–æ–±—ã —Å—É–∑–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏–∑—É—á–∞–µ–º—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ—Å—Ç–∞–Ω–∫–æ–≤.'], ..., ['–ú–∞—Ç–µ—Ä–∏–∞–ª –∏ –º–µ—Ç–æ–¥—ã.'], ['–ü—Ä–æ–≤–µ–¥–µ–Ω –ø—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–µ—á–µ–Ω–∏—è 218 –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å —Ä—É–±—Ü–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–æ–∫–æ–≤ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è, –ø–ª–æ—â–∞–¥–µ–π –ø–æ—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CO2-–ª–∞–∑–µ—Ä–∞ –≤ –ø–µ—Ä–∏–æ–¥ —Å 2011 –ø–æ 2017 –≥.'], ['–û—Ü–µ–Ω–∫—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ª–µ—á–µ–Ω–∏—è –ø—Ä–æ–≤–æ–¥–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é —à–∫–∞–ª—ã POSAS –∏ —É–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.']]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m comet_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_comet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSRC_LANG\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTGT_LANG\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(comet_score)\n",
      "File \u001b[0;32m/gpfs/home2/scur2189/Low_Resource_NMT/lib/metrics.py:44\u001b[0m, in \u001b[0;36mcompute_comet\u001b[0;34m(src, tgt, preds)\u001b[0m\n\u001b[1;32m     41\u001b[0m references \u001b[38;5;241m=\u001b[39m [[ref] \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m tgt]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# compute COMET score\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcomet_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# The results typically contain {'comet': <float_value>} or {'mean_score': <float_value>}\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# depending on the version of the metric. Adjust accordingly:\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/evaluate/module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m     )\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Module inputs don't match the expected format.\nExpected format: {'sources': Value(dtype='string', id='sequence'), 'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput sources: ['In the cases of a more rigorous thermal impact when the bone tissue exhibits well pronounced signs of heat destruction, it should be considered as inherently unsuitable for genotyping of mtDNA.', 'It was shown that chromosomal DNA is inferior to mtDNA in terms of heat resistance.  This finding agrees with the currently adopted view, however this advantage of mtDNA is relatively insignificant from the standpoint of genotyping efficiency.', '[The dynamics of the dimensional characteristics of the sella turcica in the subjects above 20 years of age].  The objective of the present study was to determine the biological age of the unidentified dead subjects based on the morphometric characteristics of the sella turcica in comparison with other methods available for the purpose in order to narrow the range of the alleged ages of the human remains being examined.', ..., 'PATIENTS AND METHODS', 'It has been carried out a prospective analysis of treatment of 218 patients with scars of different duration, locations and anatomic areas with the use of CO2-laser for the period 2011-2017.', 'POSAS scale and sonography were used for analysis.'],\nInput predictions: ['–í —Å–ª—É—á–∞–µ –±–æ–ª–µ–µ –∂–µ—Å—Ç–∫–æ–≥–æ —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ —É–¥–∞—Ä–∞, –∫–æ–≥–¥–∞ –∫–æ—Å—Ç–Ω–∞—è —Ç–∫–∞–Ω—å –∏–º–µ–µ—Ç —Ö–æ—Ä–æ—à–æ –∑–∞–º–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä—É—à–µ–Ω–∏—è, –µ–µ —Å–ª–µ–¥—É–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –ø–æ —Å–≤–æ–µ–π —Å—É—Ç–∏ –Ω–µ–ø—Ä–∏–≥–æ–¥–Ω—É—é –¥–ª—è –≥–Ω–∏–ª–æ–≥–æ –≥–Ω–∏–ª–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª—å–Ω–æ–≥–æ –≥–Ω–∏–ª–∏ —Å –≥–Ω–∏–ª—å—é –≥–Ω–∏–ª–æ–≥–æ —Å–º–æ–ª—ã –∏ –≥–Ω–∏–ª–æ–≥–æ —Å–º–æ–ª—ã.', '–ë—ã–ª–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ, —á—Ç–æ —Ö—Ä–æ–º–æ—Å–æ–º–Ω–∞—è –î–ù–ö —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ç–µ–ø–ª–æ—Å—Ç–æ–π–∫–æ—Å—Ç–∏ –Ω–∏–∂–µ mtDNA, —á—Ç–æ —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è —Å –ø—Ä–∏–Ω—è—Ç–æ–π –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Ç–æ—á–∫–æ–π –∑—Ä–µ–Ω–∏—è, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ mtden —è–≤–ª—è–µ—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è mut grum grum grum sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub int int int int int it sput sput', '–¶–µ–ª—å –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–ª–∞ –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–∑—Ä–∞—Å—Ç –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –º—ë—Ä—Ç–≤—ã—Ö —Å—É–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ—Ä—Ñ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ü–æ–∫–æ–ª—å–Ω–æ–π –¥—ã—Ä—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –∏–º–µ—é—â–∏–º–∏—Å—è –¥–ª—è —ç—Ç–æ–π —Ü–µ–ª–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Å —Ç–µ–º —á—Ç–æ–±—ã —Å—É–∑–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã—Ö –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø —Ö—Ä–µ–±—Ç–æ–≤ —Ö—Ä–µ–±—Ç–æ–≤–æ–π —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã —Ö–∏–∂–∏–Ω—ã.', ..., '–°–û–í–ï–©–ê–ù–ò–ï –ü–û –ü–û –ü–û–î–î–ï–†–ñ–ê–ù–ò–Æ –≠–¢–ò–• –ü–û–ú–û–©–¨ –ü–û –ü–û–ú–û–©–¨ –í –°–í–Ø–ó–ò –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –°–í–Ø–ó–Ø–ú –° –° –ü–û–ú–û–©–¨–Æ –ü–û –°–í–Ø–ó–Ø–ú –° –° –ü–û–ê–ú–ê–ú –ü–û –ü–û –°–ê–ú–ê–ú–ê–ú', '–ë—ã–ª –ø—Ä–æ–≤–µ–¥–µ–Ω –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–µ—á–µ–Ω–∏—è 218 –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å —à—Ä–∞–º–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è–º–∏ –∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–º–∏ –∑–æ–Ω–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ –ø–µ—Ä–∏–æ–¥ —Å 2011 –ø–æ 2017 –≥–æ–¥—ã –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö —à—Ä–∞–º–æ–≤ –∏ –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö —á–µ—Ä–µ–ø–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö —á–µ—Ä–µ–ø–∞—Ö.', '–°—Ü–µ–Ω–∞—Ä–∏–π –∏ —Å–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—é –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏ –ø–æ —Å—É—Ö–æ—Å—Ç–∏ –∏ —Å—É—Ö–æ—Å—Ç–∏.'],\nInput references: [['–£—Å—Ç–∞–Ω–æ–≤–∏–ª–∏, —á—Ç–æ —Ö—Ä–æ–º–æ—Å–æ–º–Ω–∞—è –î–ù–ö —É—Å—Ç—É–ø–∞–µ—Ç –≤ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º—Ç–î–ù–ö.'], ['–≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —É—Å—Ç–æ—è–≤—à–∏–º—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º, –æ–¥–Ω–∞–∫–æ, —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–µ–Ω–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è, —ç—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –º—Ç–î–ù–ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–≤–µ–ª–∏–∫–æ.'], ['–¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è - —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ø–æ–≥–∏–±—à–∏—Ö –ª–∏—Ü —Å –ø–æ–º–æ—â—å—é –º–æ—Ä—Ñ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —Å–µ–¥–ª–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Ö —Å —É–∂–µ –∏–º–µ—é—â–∏–º–∏—Å—è –º–µ—Ç–æ–¥–∞–º–∏, —á—Ç–æ–±—ã —Å—É–∑–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏–∑—É—á–∞–µ–º—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ—Å—Ç–∞–Ω–∫–æ–≤.'], ..., ['–ú–∞—Ç–µ—Ä–∏–∞–ª –∏ –º–µ—Ç–æ–¥—ã.'], ['–ü—Ä–æ–≤–µ–¥–µ–Ω –ø—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–µ—á–µ–Ω–∏—è 218 –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å —Ä—É–±—Ü–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–æ–∫–æ–≤ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è, –ø–ª–æ—â–∞–¥–µ–π –ø–æ—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CO2-–ª–∞–∑–µ—Ä–∞ –≤ –ø–µ—Ä–∏–æ–¥ —Å 2011 –ø–æ 2017 –≥.'], ['–û—Ü–µ–Ω–∫—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ª–µ—á–µ–Ω–∏—è –ø—Ä–æ–≤–æ–¥–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é —à–∫–∞–ª—ã POSAS –∏ —É–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.']]"
     ]
    }
   ],
   "source": [
    "comet_score = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "print(comet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8U1vIgwmE-1"
   },
   "source": [
    "You will find all the datasets for this project under: https://huggingface.co/sethjsa\n",
    "\n",
    "For other models, consider \"Helsinki-NLP/opus-mt-en-ru\" (general MT model), \"glazzova/translation_en_ru\" (tuned on biomedical domain), or \"facebook/m2m100_418M\" (multilingual model with 100 languages -- consider using for multilingual pivot experiments).\n",
    "\n",
    "To read more about the WMT Biomedical test data, see here: https://aclanthology.org/2022.wmt-1.69/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-jzQG5Vlh2X"
   },
   "source": [
    "# Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plzMI5gBmW_5"
   },
   "source": [
    "\n",
    "ONLY if you have GPU hours left and want to generate backtranslations with an LLM, consider using vLLM for faster generation. An example function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGqrOvTrhsms"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if using LLM for generation, consider using vllm for faster generation\n",
    "def translate_text_vllm(texts, model_name, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate texts using vllm for faster generation\n",
    "\n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        model_name: Name or path of the model (str)\n",
    "        tokenizer: Tokenizer object\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for translation\n",
    "    Returns:\n",
    "        translations: List of translated texts\n",
    "    \"\"\"\n",
    "    # Use model_name instead of model object\n",
    "    llm = vllm.LLM(\n",
    "        model=model_name,  # Changed from model to model_name\n",
    "        tokenizer=tokenizer,\n",
    "        tensor_parallel_size=1,\n",
    "        max_num_batched_tokens=max_length * batch_size\n",
    "    )\n",
    "\n",
    "    # Create sampling params\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.0,  # Equivalent to greedy decoding\n",
    "        max_tokens=max_length,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Generate translations in batches\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "\n",
    "        # Extract generated text from outputs\n",
    "        batch_translations = [output.outputs[0].text for output in outputs]\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    return translations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjAsnxOtpCzePND80zC+ew",
   "collapsed_sections": [
    "6-jzQG5Vlh2X"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
