{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuX1gxFbh1P-"
   },
   "source": [
    "# NLP 2 Project: Backtranslation for Domain Adaptation\n",
    "\n",
    "In this project, you will fine-tune a translation model by backtranslating monolingual in-domain text. You will then test performance in that domain as well as general domains.\n",
    "\n",
    "Your first task is to compare fine-tuning with backtranslation.\n",
    "Next, you will explore a method of data selection.\n",
    "Third, you will extend backtranslation, either modifying decoding, the model, or using multilingual pivots.\n",
    "Finally, you will explore your own research question.\n",
    "\n",
    "This notebook provides starter code to preprocess, fine-tune, and generate with a translation model. This is enough to get you started on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "# import vllm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDnACDAIjMay"
   },
   "source": [
    "## Preprocessing\n",
    "First, we need to tokenize our inputs. With HF Transformers, this is fairly simple and is done for you below. Here, we use the model's tokenizer to split the inputs into the model's pre-defined numerical tokens, i.e. convert text into tensors. We also need a function to convert back from tensors into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.preprocessing import preprocess_data, postprocess_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_kEixCZjyL-"
   },
   "source": [
    "## Evaluation\n",
    "During fine-tuning, we need to see how good the outputs are on our dev set. For this, we can use BLEU score (Papineni 2002). This function decodes the predicted tensor tokens, and computes the BLEU score.\n",
    "\n",
    "On our test sets, we also want to calculate an automatic metric, but on decoded text. We can use BLEU again, but also more advanced metrics like COMET. It's up to you to implement your choice of metric. We will discuss some metrics from the literature in class. It's always good to use at least 2 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.metrics import compute_comet, compute_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc8HvRkkogB"
   },
   "source": [
    "## Fine-tuning\n",
    "Now that we've tokenized our data and got our evaluation ready, we can start fine-tuning (i.e., training from a pre-trained model). This is a minimal training loop.\n",
    "\n",
    "We also need to generate at test time from a text dataset. This function involves generation without calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.training_utils import train_model, translate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwUNfHgylIcR"
   },
   "source": [
    "## Final Setup\n",
    "We now have all the ingredients to run our experiments. This is all standard training code; the interesting results come from what you do with the data. Below, we give an initial setup for getting the code running (either in Colab or on Snellius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nUi-Xho5hwWc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7500\n",
      "Dev dataset size: 1000\n",
      "Test dataset size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SRC_LANG = \"en\"\n",
    "TGT_LANG = \"ru\"\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "TRAIN_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "DEV_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "TEST_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "OUTPUT_DIR = \"./results/checkpoints\"\n",
    "\n",
    "train_dataset = load_dataset(TRAIN_DATASET_NAME)\n",
    "dev_dataset = load_dataset(DEV_DATASET_NAME)\n",
    "test_dataset = load_dataset(TEST_DATASET_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# change the splits for actual training. here, using flores-dev as training set because it's small (<1k examples)\n",
    "tokenized_train_dataset = preprocess_data(train_dataset, tokenizer, SRC_LANG, TGT_LANG, \"train\")\n",
    "tokenized_dev_dataset = preprocess_data(dev_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "# Note(jp): Here test is the same as dev\n",
    "tokenized_test_dataset = preprocess_data(test_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "\n",
    "# print sizes\n",
    "print(f\"Train dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(tokenized_dev_dataset)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_test_dataset)}\")\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train_dataset,\n",
    "    \"dev\": tokenized_dev_dataset,\n",
    "    \"test\": tokenized_test_dataset\n",
    "})\n",
    "\n",
    "# modify these as you wish; RQ3 could involve testing effects of various hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # generally speeds up training, try without it to see if it's faster for small datasets\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, # change batch sizes to fit your GPU memory and train faster\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    save_total_limit=1, # modify this to save more checkpoints\n",
    "    num_train_epochs=5, # modify this to train more epochs\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=4,\n",
    "    generation_max_length=128,\n",
    "    no_cuda=False,  # Set to False to enable GPU\n",
    "    fp16=True,      # Enable mixed precision training for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0lfvlbh0lory"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home2/scur2189/Low_Resource_NMT/lib/training_utils.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 08:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.134130</td>\n",
       "      <td>1.620910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.984017</td>\n",
       "      <td>3.719890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.452600</td>\n",
       "      <td>0.917389</td>\n",
       "      <td>4.389908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.452600</td>\n",
       "      <td>0.888093</td>\n",
       "      <td>4.720266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.000400</td>\n",
       "      <td>0.876513</td>\n",
       "      <td>4.681782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fine-tune model\n",
    "model_finetuned = train_model(MODEL_NAME, tokenized_datasets, tokenizer, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|                                                                                      | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:08<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test baseline model\n",
    "predictions = translate_text(test_dataset[\"dev\"][SRC_LANG], model, tokenizer, max_length=128, batch_size=64)\n",
    "\n",
    "# Test fine-tuned model\n",
    "# predictions_finetuned = translate_text(test_dataset[\"dev\"][SRC_LANG], model_finetuned, tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|                                                                                      | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:25<00:00,  5.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate checkpoint\n",
    "checkpoint_model_path = \"results/checkpoints/HILRfine-tuned-opus-mt-en-ru\"\n",
    "\n",
    "checkpoint_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_model_path)\n",
    "checkpoint_tokenizer = AutoTokenizer.from_pretrained(checkpoint_model_path)\n",
    "\n",
    "predictions_finetuned = translate_text(test_dataset[\"dev\"][SRC_LANG], checkpoint_model,\n",
    "                                       checkpoint_tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for baseline model: 6.714642882868273\n",
      "BLEU score for fine-tuned model: 6.034230781724797\n"
     ]
    }
   ],
   "source": [
    "bleu_baseline = compute_bleu(\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "\n",
    "bleu_finetuned = compute_bleu(\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions_finetuned)\n",
    "\n",
    "print(f\"BLEU score for baseline model: {bleu_baseline}\")\n",
    "print(f\"BLEU score for fine-tuned model: {bleu_finetuned}\")\n",
    "\n",
    "# BLEU score for baseline model: 6.71\n",
    "# BLEU fine-tuned-opus: 5.36\n",
    "# BLEU HILRfine-tuned-opus: 6.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5618f9f15546b7908c3a613bc716e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2189/.conda/envs/nmt/lib/python3.10/site-p ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61011bd2d7843bfbace2b195ac7c721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2189/.conda/envs/nmt/lib/python3.10/site-p ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET score for baseline model: 0.40039882718026637\n",
      "COMET score for fine-tuned model: 0.4117188140749931\n"
     ]
    }
   ],
   "source": [
    "comet_baseline = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        predictions)\n",
    "\n",
    "comet_finetuned = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        predictions_finetuned)\n",
    "\n",
    "print(f\"COMET score for baseline model: {comet_baseline}\")\n",
    "print(f\"COMET score for fine-tuned model: {comet_finetuned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8U1vIgwmE-1"
   },
   "source": [
    "You will find all the datasets for this project under: https://huggingface.co/sethjsa\n",
    "\n",
    "For other models, consider \"Helsinki-NLP/opus-mt-en-ru\" (general MT model), \"glazzova/translation_en_ru\" (tuned on biomedical domain), or \"facebook/m2m100_418M\" (multilingual model with 100 languages -- consider using for multilingual pivot experiments).\n",
    "\n",
    "To read more about the WMT Biomedical test data, see here: https://aclanthology.org/2022.wmt-1.69/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-jzQG5Vlh2X"
   },
   "source": [
    "# Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plzMI5gBmW_5"
   },
   "source": [
    "\n",
    "ONLY if you have GPU hours left and want to generate backtranslations with an LLM, consider using vLLM for faster generation. An example function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGqrOvTrhsms"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if using LLM for generation, consider using vllm for faster generation\n",
    "def translate_text_vllm(texts, model_name, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate texts using vllm for faster generation\n",
    "\n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        model_name: Name or path of the model (str)\n",
    "        tokenizer: Tokenizer object\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for translation\n",
    "    Returns:\n",
    "        translations: List of translated texts\n",
    "    \"\"\"\n",
    "    # Use model_name instead of model object\n",
    "    llm = vllm.LLM(\n",
    "        model=model_name,  # Changed from model to model_name\n",
    "        tokenizer=tokenizer,\n",
    "        tensor_parallel_size=1,\n",
    "        max_num_batched_tokens=max_length * batch_size\n",
    "    )\n",
    "\n",
    "    # Create sampling params\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.0,  # Equivalent to greedy decoding\n",
    "        max_tokens=max_length,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Generate translations in batches\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "\n",
    "        # Extract generated text from outputs\n",
    "        batch_translations = [output.outputs[0].text for output in outputs]\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    return translations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjAsnxOtpCzePND80zC+ew",
   "collapsed_sections": [
    "6-jzQG5Vlh2X"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
