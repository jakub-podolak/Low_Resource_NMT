{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuX1gxFbh1P-"
   },
   "source": [
    "# NLP 2 Project: Backtranslation for Domain Adaptation\n",
    "\n",
    "In this project, you will fine-tune a translation model by backtranslating monolingual in-domain text. You will then test performance in that domain as well as general domains.\n",
    "\n",
    "Your first task is to compare fine-tuning with backtranslation.\n",
    "Next, you will explore a method of data selection.\n",
    "Third, you will extend backtranslation, either modifying decoding, the model, or using multilingual pivots.\n",
    "Finally, you will explore your own research question.\n",
    "\n",
    "This notebook provides starter code to preprocess, fine-tune, and generate with a translation model. This is enough to get you started on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "# import vllm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDnACDAIjMay"
   },
   "source": [
    "## Preprocessing\n",
    "First, we need to tokenize our inputs. With HF Transformers, this is fairly simple and is done for you below. Here, we use the model's tokenizer to split the inputs into the model's pre-defined numerical tokens, i.e. convert text into tensors. We also need a function to convert back from tensors into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.preprocessing import preprocess_data, postprocess_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_kEixCZjyL-"
   },
   "source": [
    "## Evaluation\n",
    "During fine-tuning, we need to see how good the outputs are on our dev set. For this, we can use BLEU score (Papineni 2002). This function decodes the predicted tensor tokens, and computes the BLEU score.\n",
    "\n",
    "On our test sets, we also want to calculate an automatic metric, but on decoded text. We can use BLEU again, but also more advanced metrics like COMET. It's up to you to implement your choice of metric. We will discuss some metrics from the literature in class. It's always good to use at least 2 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.metrics import compute_comet, compute_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc8HvRkkogB"
   },
   "source": [
    "## Fine-tuning\n",
    "Now that we've tokenized our data and got our evaluation ready, we can start fine-tuning (i.e., training from a pre-trained model). This is a minimal training loop.\n",
    "\n",
    "We also need to generate at test time from a text dataset. This function involves generation without calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.training_utils import train_model, translate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwUNfHgylIcR"
   },
   "source": [
    "## Final Setup\n",
    "We now have all the ingredients to run our experiments. This is all standard training code; the interesting results come from what you do with the data. Below, we give an initial setup for getting the code running (either in Colab or on Snellius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train from disk\n"
     ]
    }
   ],
   "source": [
    "SRC_LANG = \"en\"\n",
    "TGT_LANG = \"ru\"\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "TRAIN_DATASET_NAME = \"results/datasets/sethjsa_medline_ru_mono_backtranslated_with_Helsinki-NLP_opus-mt-ru-en.json\" # \"sethjsa/medline_en_ru_parallel\"\n",
    "DEV_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "TEST_DATASET_NAME = \"sethjsa/medline_en_ru_parallel\"\n",
    "OUTPUT_DIR = \"./results/checkpoints\"\n",
    "\n",
    "if TRAIN_DATASET_NAME.startswith(\"sethjsa\"):\n",
    "    train_dataset = load_dataset(TRAIN_DATASET_NAME)\n",
    "else:\n",
    "    print(f\"Loading train from disk\")\n",
    "    train_dataset = load_from_disk(TRAIN_DATASET_NAME)\n",
    "dev_dataset = load_dataset(DEV_DATASET_NAME)\n",
    "test_dataset = load_dataset(TEST_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nUi-Xho5hwWc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc63f44dfd3148abac2b4dd90fb4611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 100\n",
      "Dev dataset size: 1000\n",
      "Test dataset size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# change the splits for actual training. here, using flores-dev as training set because it's small (<1k examples)\n",
    "tokenized_train_dataset = preprocess_data(train_dataset, tokenizer, SRC_LANG, TGT_LANG, \"train\")\n",
    "tokenized_dev_dataset = preprocess_data(dev_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "# Note(jp): Here test is the same as dev\n",
    "tokenized_test_dataset = preprocess_data(test_dataset, tokenizer, SRC_LANG, TGT_LANG, \"dev\")\n",
    "\n",
    "# print sizes\n",
    "print(f\"Train dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(tokenized_dev_dataset)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_test_dataset)}\")\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train_dataset,\n",
    "    \"dev\": tokenized_dev_dataset,\n",
    "    \"test\": tokenized_test_dataset\n",
    "})\n",
    "\n",
    "# {'learning_rate': 0.0001, 'weight_decay': 0.05, 'num_train_epochs': 8}\n",
    "# modify these as you wish; RQ3 could involve testing effects of various hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # generally speeds up training, try without it to see if it's faster for small datasets\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64, # change batch sizes to fit your GPU memory and train faster\n",
    "    per_device_eval_batch_size=128,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    save_total_limit=1, # modify this to save more checkpoints\n",
    "    num_train_epochs=5, # modify this to train more epochs\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=4,\n",
    "    generation_max_length=128,\n",
    "    warmup_steps=0,\n",
    "    no_cuda=False,  # Set to False to enable GPU\n",
    "    fp16=True,      # Enable mixed precision training for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0lfvlbh0lory"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fine-tune model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_finetuned \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home2/scur2189/Low_Resource_NMT/lib/training_utils.py:35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, tokenized_datasets, tokenizer, training_args)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model_name, tokenized_datasets, tokenizer, training_args):\n\u001b[0;32m---> 35\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Verify GPU usage\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/modeling_utils.py:4185\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4179\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4180\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4181\u001b[0m     )\n\u001b[1;32m   4183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   4184\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4185\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4187\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   4188\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1240\u001b[0m, in \u001b[0;36mMarianMTModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: MarianConfig):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mMarianModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     target_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mshare_encoder_decoder_embeddings \u001b[38;5;28;01melse\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdecoder_vocab_size\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_logits_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, target_vocab_size)))\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1062\u001b[0m, in \u001b[0;36mMarianModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m MarianEncoder(config, encoder_embed_tokens)\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mMarianDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_embed_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:813\u001b[0m, in \u001b[0;36mMarianDecoder.__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mdecoder_vocab_size, config\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions \u001b[38;5;241m=\u001b[39m \u001b[43mMarianSinusoidalPositionalEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([MarianDecoderLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:76\u001b[0m, in \u001b[0;36mMarianSinusoidalPositionalEmbedding.__init__\u001b[0;34m(self, num_positions, embedding_dim, padding_idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_positions: \u001b[38;5;28mint\u001b[39m, embedding_dim: \u001b[38;5;28mint\u001b[39m, padding_idx: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(num_positions, embedding_dim)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:86\u001b[0m, in \u001b[0;36mMarianSinusoidalPositionalEmbedding._init_weight\u001b[0;34m(out)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mIdentical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mthe 2nd half of the vector. [dim // 2:]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m n_pos, dim \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     85\u001b[0m position_enc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m---> 86\u001b[0m     [[pos \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m dim) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim)] \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_pos)]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m out\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# set early to avoid an error in pytorch-1.8+\u001b[39;00m\n\u001b[1;32m     89\u001b[0m sentinel \u001b[38;5;241m=\u001b[39m dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mIdentical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mthe 2nd half of the vector. [dim // 2:]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m n_pos, dim \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     85\u001b[0m position_enc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m---> 86\u001b[0m     [[pos \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m dim) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim)] \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_pos)]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m out\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# set early to avoid an error in pytorch-1.8+\u001b[39;00m\n\u001b[1;32m     89\u001b[0m sentinel \u001b[38;5;241m=\u001b[39m dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nmt/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mIdentical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mthe 2nd half of the vector. [dim // 2:]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m n_pos, dim \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     85\u001b[0m position_enc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m---> 86\u001b[0m     [[pos \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim)] \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_pos)]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m out\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# set early to avoid an error in pytorch-1.8+\u001b[39;00m\n\u001b[1;32m     89\u001b[0m sentinel \u001b[38;5;241m=\u001b[39m dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fine-tune model\n",
    "model_finetuned = train_model(MODEL_NAME, tokenized_datasets, tokenizer, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on TICO test 10 for baseline and around 14 for fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|                                                                                                          | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:08<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test baseline model\n",
    "predictions = translate_text(test_dataset[\"dev\"][SRC_LANG], model, tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:29<00:00,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate checkpoint\n",
    "# checkpoint_model_path = \"results/checkpoints/final_model/best-tuned-opus-mt-en-ru\"\n",
    "\n",
    "# checkpoint_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_model_path)\n",
    "# checkpoint_tokenizer = AutoTokenizer.from_pretrained(checkpoint_model_path)\n",
    "\n",
    "predictions_finetuned = translate_text(test_dataset[\"dev\"][SRC_LANG], model_finetuned,\n",
    "                                       tokenizer, max_length=128, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and are you having any of the following symptoms with your chest pain \n",
      "Ð¸ Ð¸Ð¼ÐµÑŽÑ‚ÑÑ Ð»Ð¸ Ñƒ Ð²Ð°Ñ ÑÐµÐ¹Ñ‡Ð°Ñ ÐºÐ°ÐºÐ¸Ðµ-Ð»Ð¸Ð±Ð¾ Ð¸Ð· ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… ÑÐ¸Ð¼Ð¿Ñ‚Ð¾Ð¼Ð¾Ð² Ð¿Ñ€Ð¸ Ð±Ð¾Ð»Ð¸ Ð² Ð³Ñ€ÑƒÐ´Ð½Ð¾Ð¹ ÐºÐ»ÐµÑ‚ÐºÐµ\n",
      "Ð¸ Ð²Ñ‹ Ð¸Ð¼ÐµÐµÑ‚Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ¸Ð¼Ð¿Ñ‚Ð¾Ð¼Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²Ñ‹ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚Ðµ: \"Ð“Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚Ðµ Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹ Ð³Ñ€ÑƒÐ´Ð¸\" Ð¸ \"Ð“Ð»ÑƒÐ±Ð¸Ð½Ð°\" - Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹ (Ð“Ð»ÑƒÐ±Ð¸Ð½Ð½Ð¾Ð¹) (Ð“Ð»Ð°Ð²Ð½Ð¾Ð¹) (Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹) Ð²Ð¸ÑÐºÐ¸ (Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹) (Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹) Ð²Ð¸ÑÐºÐ¸ (Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹) Ð² Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð¸ (Ð¿Ð¾ Ð²Ð¾Ð»ÑˆÐµÐ±Ð½Ð¾Ð¹) Ð² Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð¸ (Ð¿Ð¾ Ð²ÐµÐ»Ð¸Ñ‡Ð¸ÑŽ) (Ð¿Ð¾ Ð²ÐµÐ»Ð¸Ñ‡Ð¸ÑŽ) (Ð¿Ð¾ Ð²\n",
      "Ð’ Ñ…Ñ€Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð³Ñ€ÑƒÐ¿Ð¿Ðµ Ð±Ð¾Ð»ÑŒÐ½Ñ‹Ñ… Ð² Ð³Ñ€ÑƒÐ´Ð½Ð¾Ð¹ Ð³Ñ€ÑƒÐ¿Ð¿Ðµ Ð¿Ñ€ÐµÐ¿Ð°Ñ€Ð°Ñ‚Ð¾Ð² Ñ Ñ…Ñ€Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÑÐ¸Ð¼Ð¿Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒÑŽ Ð¸ Ñ…Ñ€Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÑÐ¸Ð¼Ð¿Ñ‚\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[\"dev\"][SRC_LANG][0])\n",
    "print(test_dataset[\"dev\"][TGT_LANG][0])\n",
    "print(predictions[0])\n",
    "print(predictions_finetuned[0])\n",
    "# Ð’ ÑÐ»ÑƒÑ‡Ð°ÑÑ… Ð±Ð¾Ð»ÐµÐµ Ñ‚ÑÐ¶ÐµÐ»Ð¾Ð³Ð¾ Ñ‚ÐµÑ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ð¾Ð·Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, Ð¿Ñ€Ð¸ ÐºÐ¾ÑÑ‚Ð½Ð¾Ð¹ Ñ‚ÐºÐ°Ð½Ð¸ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ñ‚ÐµÑ€Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ð¾Ð·Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, Ð¿Ñ€Ð¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for baseline model: 6.714642882868273\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bleu_finetuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bleu_finetuned = compute_bleu(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#         test_dataset[\"dev\"][TGT_LANG],\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         predictions_finetuned)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score for baseline model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_baseline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score for fine-tuned model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbleu_finetuned\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# WMT\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# BLEU score for baseline model: 6.714642882868273\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# BLEU score for fine-tuned model: 6.379819414189476\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_finetuned' is not defined"
     ]
    }
   ],
   "source": [
    "bleu_baseline = compute_bleu(\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "\n",
    "bleu_finetuned = compute_bleu(\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions_finetuned)\n",
    "\n",
    "print(f\"BLEU score for baseline model: {bleu_baseline}\")\n",
    "print(f\"BLEU score for fine-tuned model: {bleu_finetuned}\")\n",
    "\n",
    "# WMT\n",
    "# BLEU score for baseline model: 6.714642882868273\n",
    "# BLEU score for fine-tuned model: 6.379819414189476\n",
    "\n",
    "# TICO\n",
    "# BLEU score for baseline model: 12.522936666648125\n",
    "# BLEU score for fine-tuned model: 3.078057409647383\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07df7888125a495889c4c0691d37d0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2189/.conda/envs/nmt/lib/python3.10/site-p ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3548f6c2ed74babb9c4700725e06097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2189/.conda/envs/nmt/lib/python3.10/site-p ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET score for baseline model: 0.5964977323671325\n",
      "COMET score for fine-tuned model: 0.4390196953080096\n"
     ]
    }
   ],
   "source": [
    "comet_baseline = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions)\n",
    "\n",
    "comet_finetuned = compute_comet(\n",
    "        test_dataset[\"dev\"][SRC_LANG],\n",
    "        test_dataset[\"dev\"][TGT_LANG],\n",
    "        predictions_finetuned)\n",
    "\n",
    "print(f\"COMET score for baseline model: {comet_baseline}\")\n",
    "print(f\"COMET score for fine-tuned model: {comet_finetuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the cases of a more rigorous thermal impact when the bone tissue exhibits well pronounced signs of heat destruction, it should be considered as inherently unsuitable for genotyping of mtDNA.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"dev\"][SRC_LANG][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ñ…Ñ€Ð¾Ð¼Ð¾ÑÐ¾Ð¼Ð½Ð°Ñ Ð”ÐÐš ÑƒÑÑ‚ÑƒÐ¿Ð°ÐµÑ‚ Ð² Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚Ð¸ Ð¼Ñ‚Ð”ÐÐš.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"dev\"][TGT_LANG][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ð’ ÑÐ»ÑƒÑ‡Ð°Ðµ Ð±Ð¾Ð»ÐµÐµ Ð¶ÐµÑÑ‚ÐºÐ¾Ð³Ð¾ Ñ‚ÐµÑ€Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑƒÐ´Ð°Ñ€Ð°, ÐºÐ¾Ð³Ð´Ð° ÐºÐ¾ÑÑ‚Ð½Ð°Ñ Ñ‚ÐºÐ°Ð½ÑŒ Ð¸Ð¼ÐµÐµÑ‚ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð·Ð°Ð¼ÐµÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ñ‚ÐµÑ€Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ€Ð°Ð·Ñ€ÑƒÑˆÐµÐ½Ð¸Ñ, ÐµÐµ ÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°Ñ‚ÑŒ ÐºÐ°Ðº Ð¿Ð¾ ÑÐ²Ð¾ÐµÐ¹ ÑÑƒÑ‚Ð¸ Ð½ÐµÐ¿Ñ€Ð¸Ð³Ð¾Ð´Ð½ÑƒÑŽ Ð´Ð»Ñ Ð³Ð½Ð¸Ð»Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð³Ð½Ð¸Ð»Ð¸ Ñ Ð³Ð½Ð¸Ð»ÑŒÑŽ Ð³Ð½Ð¸Ð»Ð¾Ð³Ð¾ ÑÐ¼Ð¾Ð»Ñ‹ Ð¸ Ð³Ð½Ð¸Ð»Ð¾Ð³Ð¾ ÑÐ¼Ð¾Ð»Ñ‹.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ð’ ÑÐ»ÑƒÑ‡Ð°ÑÑ… Ð±Ð¾Ð»ÐµÐµ Ñ‚ÑÐ¶ÐµÐ»Ð¾Ð³Ð¾ Ñ‚ÐµÑ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ð¾Ð·Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, Ð¿Ñ€Ð¸ ÐºÐ¾ÑÑ‚Ð½Ð¾Ð¹ Ñ‚ÐºÐ°Ð½Ð¸ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ñ‚ÐµÑ€Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ð¾Ð·Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, Ð¿Ñ€Ð¸'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_finetuned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate==0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep \"evaluate\" # evaluate==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading checkpoint model from: results/checkpoints/final_model/best-tuned-opus-mt-en-ru\n",
      "[INFO] Loading datasets...\n",
      "[INFO] Loading dataset: sethjsa/medline_en_ru_parallel (dev)\n",
      "[INFO] Preprocessing data...\n",
      "[INFO] Translating test data...\n",
      "Translating:   0%|                                       | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:25<00:00,  5.32s/it]\n",
      "[INFO] Computing metrics...\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 41282.52it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 scripts/evaluate_test.py --checkpoint_path results/ ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[INFO] BLEU score: 6.604879770209201\n",
      "[INFO] COMET score: 0.5222889098227024\n",
      "[INFO] Results:\n",
      "Dataset: sethjsa/medline_en_ru_parallel, Split: dev, Samples: 1000, BLEU: 6.604879770209201, COMET: 0.5222889098227024\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/evaluate_test.py --checkpoint_path results/checkpoints/final_model/best-tuned-opus-mt-en-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading baseline model from: Helsinki-NLP/opus-mt-en-ru\n",
      "[INFO] Loading datasets...\n",
      "[INFO] Loading dataset: sethjsa/medline_en_ru_parallel (dev)\n",
      "[INFO] Preprocessing data...\n",
      "[INFO] Translating test data...\n",
      "Translating:   0%|                                       | 0/16 [00:00<?, ?it/s]/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:08<00:00,  4.26s/it]\n",
      "[INFO] Computing metrics...\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 24188.60it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../home/scur2189/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/scur2189/.conda/envs/nmt/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 scripts/evaluate_test.py ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[INFO] BLEU score: 6.714642882868273\n",
      "[INFO] COMET score: 0.4779623000472784\n",
      "[INFO] Results:\n",
      "Dataset: sethjsa/medline_en_ru_parallel, Split: dev, Samples: 1000, BLEU: 6.714642882868273, COMET: 0.4779623000472784\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/evaluate_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8U1vIgwmE-1"
   },
   "source": [
    "You will find all the datasets for this project under: https://huggingface.co/sethjsa\n",
    "\n",
    "For other models, consider \"Helsinki-NLP/opus-mt-en-ru\" (general MT model), \"glazzova/translation_en_ru\" (tuned on biomedical domain), or \"facebook/m2m100_418M\" (multilingual model with 100 languages -- consider using for multilingual pivot experiments).\n",
    "\n",
    "To read more about the WMT Biomedical test data, see here: https://aclanthology.org/2022.wmt-1.69/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-jzQG5Vlh2X"
   },
   "source": [
    "# Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plzMI5gBmW_5"
   },
   "source": [
    "\n",
    "ONLY if you have GPU hours left and want to generate backtranslations with an LLM, consider using vLLM for faster generation. An example function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGqrOvTrhsms"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if using LLM for generation, consider using vllm for faster generation\n",
    "def translate_text_vllm(texts, model_name, tokenizer, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate texts using vllm for faster generation\n",
    "\n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        model_name: Name or path of the model (str)\n",
    "        tokenizer: Tokenizer object\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for translation\n",
    "    Returns:\n",
    "        translations: List of translated texts\n",
    "    \"\"\"\n",
    "    # Use model_name instead of model object\n",
    "    llm = vllm.LLM(\n",
    "        model=model_name,  # Changed from model to model_name\n",
    "        tokenizer=tokenizer,\n",
    "        tensor_parallel_size=1,\n",
    "        max_num_batched_tokens=max_length * batch_size\n",
    "    )\n",
    "\n",
    "    # Create sampling params\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.0,  # Equivalent to greedy decoding\n",
    "        max_tokens=max_length,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Generate translations in batches\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "\n",
    "        # Extract generated text from outputs\n",
    "        batch_translations = [output.outputs[0].text for output in outputs]\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    return translations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjAsnxOtpCzePND80zC+ew",
   "collapsed_sections": [
    "6-jzQG5Vlh2X"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
